{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comparative Retrieval Analysis -- Affective-RAG\n\nCompares five retrieval configurations on agreement and dissonance query sets:\n\n| Method | Description |\n|--------|-------------|\n| BM25 | Lexical baseline |\n| Vector-RAG | Semantic + emotion, no knowledge graph |\n| KRAG ($\\alpha$=1.0) | Relevance-dominated |\n| KRAG ($\\alpha$=0.5) | Balanced |\n| KRAG ($\\alpha$=0.3) | Affective-focused |\n\nEvaluates AP@5, ADE, and Semantic Recall on 800 agreement + 800 dissonance queries.\nIncludes paired statistical tests (Cohen's $d_z$, 95% CIs).\n\n### Prerequisites\n| # | Requirement |\n|---|-------------|\n| 1 | Runtime set to **GPU** |\n| 2 | GCP project with GCS bucket containing the dataset |\n| 3 | Trained encoder checkpoint (`krag_encoder.pt`) to upload |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Install & clone"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\ndef _pip(*a):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + list(a) + [\"-q\"])\n\n_pip('torch-geometric')\n\nREPO_URL = \"https://github.com/Prashant002-1/Affective-RAG-Recommender-Systems.git\"\n!git clone {REPO_URL} ARAG\n\n_pip(\"-r\", \"ARAG/requirements.txt\")\nprint(\"Done. Restart runtime.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Restart the runtime\nGo to **Runtime > Restart runtime**, then continue from the next cell."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Authenticate to GCS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import auth\nauth.authenticate_user()\n\nimport os\nPROJECT_ID = \"YOUR_PROJECT_ID\"          # <-- set this\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n\nfrom google.cloud import storage\nstorage.Client(project=PROJECT_ID)\nprint(f\"Authenticated: {PROJECT_ID}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Upload trained encoder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab.files import upload\nprint(\"Upload trained encoder checkpoint (krag_encoder.pt)\")\nuploaded = upload()\nENCODER_PATH = list(uploaded.keys())[0]\nprint(f\"Uploaded: {ENCODER_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, torch, json, numpy as np\nfrom scipy import stats as sp_stats\nsys.path.insert(0, \"ARAG/src\")\n\nfrom krag.system               import ARAGSystem, ARAGSystemConfig\nfrom krag.training.gnn_trainer  import prepare_emotion_ground_truth\nfrom krag.data.adapters         import get_adapter\nfrom krag.evaluation.synthetic_testset import SyntheticTestSetGenerator\nfrom krag.evaluation.metrics    import (\n    compute_affective_precision_at_k,\n    compute_affective_displacement_error,\n    compute_semantic_recall_at_k,\n)\nfrom krag.retrieval.bm25_retriever  import create_bm25_retriever_from_content_items\nfrom krag.retrieval.krag_retriever  import QueryContext\nfrom krag.core.emotion_detection    import EmotionProfile\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Imports OK | device = {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialize system"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config = ARAGSystemConfig()\nconfig.vertex_ai_project = PROJECT_ID\nsystem = ARAGSystem(config)\nsystem.initialize()\nstats  = system.load_and_index_data()\nprint(f\"Content items : {stats['content_items']}\")\nprint(f\"KG nodes      : {stats['knowledge_graph_nodes']}\")\nprint(f\"KG edges      : {stats['knowledge_graph_edges']}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load encoder and re-index at k=2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "system.krag_encoder.load_state_dict(\n    torch.load(ENCODER_PATH, map_location=DEVICE)\n)\nsystem.krag_encoder.to(DEVICE)\nsystem.krag_encoder.eval()\n\nsystem.response_generator.generate_response = lambda *a, **kw: \"\"\n\ncontent_ids = [str(item.id) for item in system.content_items]\nsystem.subgraph_retriever.subgraph_embeddings = {}\nsystem.subgraph_retriever.index_subgraphs(content_ids, hops=2)\nprint(\"Encoder loaded, subgraphs re-indexed at k=2.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load evaluation data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "adapter = get_adapter()\nmovies_df = adapter.load_movies(vector_ready=True)\naff_sigs  = prepare_emotion_ground_truth(movies_df)\nprint(f\"Affective signatures : {len(aff_sigs)}\")\n\n_result = system.vector_store.semantic_collection.get(include=[\"embeddings\"])\n_emb_map = dict(zip(_result[\"ids\"], _result[\"embeddings\"]))\ncontent_embeddings = np.array([_emb_map[cid] for cid in content_ids])\nprint(f\"Content embeddings   : {content_embeddings.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Generate test sets\n\n800 agreement queries and 800 dissonance queries.  Thresholds and seed\nmatch the original comparative run (semantic = 0.5, affective = 0.6).\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "gen = SyntheticTestSetGenerator(\n    content_embedder=system.content_embedder,\n    semantic_threshold=0.5,\n    affective_threshold=0.6,\n    seed=42,\n)\n\nprint(\"[1/2] Agreement queries …\")\nagreement = gen.generate_test_set(\n    content_items=system.content_items,\n    content_embeddings=content_embeddings,\n    movie_affective_signatures=aff_sigs,\n    num_queries=800,\n    min_relevant=3,\n)\nprint(f\"      {len(agreement)} queries\")\n\nprint(\"[2/2] Dissonance queries …\")\ndissonance = gen.generate_dissonance_queries(\n    content_items=system.content_items,\n    content_embeddings=content_embeddings,\n    movie_affective_signatures=aff_sigs,\n    num_queries=800,\n)\nprint(f\"      {len(dissonance)} queries\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Create retrievers\n\nBM25 is created directly from content items.  The KRAG and Vector-RAG\nvariants are switched in via the system so they share the same indexed\ndata and encoder.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# BM25 — created manually (needs content_items, not available via set_retriever_type)\nbm25_retriever = create_bm25_retriever_from_content_items(system.content_items)\n\n# Retriever configs for the system-based loop:\n#   (label, retriever_type, alpha | None)\nSYSTEM_CONFIGS = [\n    (\"Vector-RAG\",    \"vector_only\",  None),\n    (\"KRAG (a=1.0)\",  \"krag\",        1.0),\n    (\"KRAG (a=0.5)\",  \"krag\",        0.5),\n    (\"KRAG (a=0.3)\",  \"krag\",        0.3),\n]\nprint(\"Retrievers ready.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluation\n\nBM25 is evaluated via its own retrieve path.  The remaining four\nconfigurations are run through `system.get_recommendations` after\nswitching the active retriever.  Per-query AP@5 and ADE scores are\nretained for the paired statistical tests.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "EMOTION_ORDER = [\"happiness\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"disgust\"]\n\ndef to_vec(target_emotions):\n    if isinstance(target_emotions, dict):\n        return np.array([target_emotions.get(e, 0.0) for e in EMOTION_ORDER])\n    return np.asarray(target_emotions)\n\ndef to_sliders(target_emotions):\n    vec = to_vec(target_emotions)\n    return {e: int(round(v * 10)) for e, v in zip(EMOTION_ORDER, vec.tolist())}\n\ndef _make_query_context(query_text):\n    return QueryContext(\n        query_text=query_text,\n        user_emotions=EmotionProfile(),\n        query_embedding=np.zeros(768),\n        emotion_embedding=np.zeros(768),\n    )\n\n# ── BM25 evaluation helper ─────────────────────────────────\ndef eval_bm25(test_cases):\n    sem_r5, sem_r10, ap5s, ap10s, ades = [], [], [], [], []\n    for tc in test_cases:\n        target_vec = to_vec(tc.target_emotions)\n        try:\n            qc  = _make_query_context(tc.query_text)\n            raw = bm25_retriever.retrieve(qc, k=10)\n            retrieved_ids = [r.content_id for r in raw]\n        except Exception as e:\n            print(f\"  [BM25] {e}\")\n            continue\n        if not retrieved_ids:\n            continue\n\n        sem_r5.append(compute_semantic_recall_at_k(retrieved_ids, tc.semantic_relevant or [], 5))\n        sem_r10.append(compute_semantic_recall_at_k(retrieved_ids, tc.semantic_relevant or [], 10))\n        ap5s.append(compute_affective_precision_at_k(retrieved_ids, aff_sigs, target_vec, 5))\n        ap10s.append(compute_affective_precision_at_k(retrieved_ids, aff_sigs, target_vec, 10))\n        ade = compute_affective_displacement_error(retrieved_ids, aff_sigs, target_vec, 10)\n        if ade != float('inf'):\n            ades.append(ade)\n\n    return {\n        \"Semantic_Recall@5\":  float(np.mean(sem_r5))  if sem_r5  else 0.0,\n        \"Semantic_Recall@10\": float(np.mean(sem_r10)) if sem_r10 else 0.0,\n        \"AP@5\":               float(np.mean(ap5s))    if ap5s    else 0.0,\n        \"AP@10\":              float(np.mean(ap10s))   if ap10s   else 0.0,\n        \"ADE\":                float(np.mean(ades))    if ades    else float('inf'),\n    }, {\"AP@5\": ap5s, \"ADE\": ades}\n\n# ── system-retriever evaluation helper ─────────────────────\ndef eval_system(test_cases, rtype, alpha):\n    if alpha is not None:\n        system.config.alpha = alpha\n    system.set_retriever_type(rtype)\n\n    sem_r5, sem_r10, ap5s, ap10s, ades = [], [], [], [], []\n    for tc in test_cases:\n        target_vec = to_vec(tc.target_emotions)\n        sliders    = to_sliders(tc.target_emotions)\n\n        recs  = system.query(\n            query_text=tc.query_text, emotion_sliders=sliders,\n            max_results=10, include_explanation=False\n        )\n        items = recs.get(\"recommendations\", [])\n        if not items:\n            continue\n        retrieved_ids = [r[\"content_id\"] for r in items]\n\n        sem_r5.append(compute_semantic_recall_at_k(retrieved_ids, tc.semantic_relevant or [], 5))\n        sem_r10.append(compute_semantic_recall_at_k(retrieved_ids, tc.semantic_relevant or [], 10))\n        ap5s.append(compute_affective_precision_at_k(retrieved_ids, aff_sigs, target_vec, 5))\n        ap10s.append(compute_affective_precision_at_k(retrieved_ids, aff_sigs, target_vec, 10))\n        ade = compute_affective_displacement_error(retrieved_ids, aff_sigs, target_vec, 10)\n        if ade != float('inf'):\n            ades.append(ade)\n\n    return {\n        \"Semantic_Recall@5\":  float(np.mean(sem_r5))  if sem_r5  else 0.0,\n        \"Semantic_Recall@10\": float(np.mean(sem_r10)) if sem_r10 else 0.0,\n        \"AP@5\":               float(np.mean(ap5s))    if ap5s    else 0.0,\n        \"AP@10\":              float(np.mean(ap10s))   if ap10s   else 0.0,\n        \"ADE\":                float(np.mean(ades))    if ades    else float('inf'),\n    }, {\"AP@5\": ap5s, \"ADE\": ades}\n\nprint(\"Evaluation helpers defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run — Agreement set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "system.response_generator.generate_response = lambda *a, **kw: \"\"\n\nprint(\"=== Agreement (n={}) ===\\n\".format(len(agreement)))\n\nagr_results  = {}\nagr_per_query = {}\n\n# BM25\nprint(\"BM25 …\", flush=True)\nagr_results[\"BM25\"], agr_per_query[\"BM25\"] = eval_bm25(agreement)\nprint(f\"  AP@5={agr_results['BM25']['AP@5']:.4f}  ADE={agr_results['BM25']['ADE']:.4f}\")\n\n# System-based retrievers\nfor name, rtype, alpha in SYSTEM_CONFIGS:\n    print(f\"{name} …\", flush=True)\n    agr_results[name], agr_per_query[name] = eval_system(agreement, rtype, alpha)\n    print(f\"  AP@5={agr_results[name]['AP@5']:.4f}  ADE={agr_results[name]['ADE']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run — Dissonance set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "system.response_generator.generate_response = lambda *a, **kw: \"\"\n\nprint(\"=== Dissonance (n={}) ===\\n\".format(len(dissonance)))\n\ndis_results  = {}\ndis_per_query = {}\n\nprint(\"BM25 …\", flush=True)\ndis_results[\"BM25\"], dis_per_query[\"BM25\"] = eval_bm25(dissonance)\nprint(f\"  AP@5={dis_results['BM25']['AP@5']:.4f}  ADE={dis_results['BM25']['ADE']:.4f}\")\n\nfor name, rtype, alpha in SYSTEM_CONFIGS:\n    print(f\"{name} …\", flush=True)\n    dis_results[name], dis_per_query[name] = eval_system(dissonance, rtype, alpha)\n    print(f\"  AP@5={dis_results[name]['AP@5']:.4f}  ADE={dis_results[name]['ADE']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Statistical tests\n\nPaired comparison between KRAG α=0.3 and KRAG α=1.0.\nReports mean paired difference, 95% CI, and Cohen's $d_z$.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def paired_stats(a_scores, b_scores, label_a, label_b, metric):\n    a = np.array(a_scores)\n    b = np.array(b_scores)\n    n = min(len(a), len(b))\n    a, b = a[:n], b[:n]\n    diff = a - b\n    mean_d  = float(np.mean(diff))\n    pooled  = np.sqrt((np.var(a, ddof=1) + np.var(b, ddof=1)) / 2)\n    dz      = mean_d / pooled if pooled > 0 else 0.0\n    se      = np.std(diff, ddof=1) / np.sqrt(n)\n    t_crit  = sp_stats.t.ppf(0.975, df=n - 1)\n    ci      = (mean_d - t_crit * se, mean_d + t_crit * se)\n    print(f\"  {metric:6s}  diff={mean_d:+.4f}  95% CI [{ci[0]:+.4f}, {ci[1]:+.4f}]  dz={dz:+.2f}  n={n}\")\n    return {\"mean_diff\": mean_d, \"ci\": list(ci), \"dz\": dz, \"n\": n}\n\nstat = {}\n\nprint(\"\\nAgreement  (a=0.3 minus a=1.0)\")\nstat[\"agreement\"] = {}\nfor m in [\"AP@5\", \"ADE\"]:\n    stat[\"agreement\"][m] = paired_stats(\n        agr_per_query[\"KRAG (a=0.3)\"][m],\n        agr_per_query[\"KRAG (a=1.0)\"][m],\n        \"a=0.3\", \"a=1.0\", m,\n    )\n\nprint(\"\\nDissonance  (a=0.3 minus a=1.0)\")\nstat[\"dissonance\"] = {}\nfor m in [\"AP@5\", \"ADE\"]:\n    stat[\"dissonance\"][m] = paired_stats(\n        dis_per_query[\"KRAG (a=0.3)\"][m],\n        dis_per_query[\"KRAG (a=1.0)\"][m],\n        \"a=0.3\", \"a=1.0\", m,\n    )\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Plots"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nNAMES  = [\"BM25\", \"Vector-RAG\", \"KRAG (a=1.0)\", \"KRAG (a=0.5)\", \"KRAG (a=0.3)\"]\nx      = np.arange(len(NAMES))\nwidth  = 0.6\n\nplots = [\n    (\"agr_ap5\",  agr_results, \"AP@5\",                \"Agreement — Affective Precision@5\", \"steelblue\"),\n    (\"agr_ade\",  agr_results, \"ADE\",                 \"Agreement — Displacement Error\",    \"steelblue\"),\n    (\"agr_sr10\", agr_results, \"Semantic_Recall@10\",   \"Agreement — Semantic Recall@10\",    \"steelblue\"),\n    (\"dis_ap5\",  dis_results, \"AP@5\",                \"Dissonance — Affective Precision@5\", \"darkorange\"),\n    (\"dis_ade\",  dis_results, \"ADE\",                 \"Dissonance — Displacement Error\",    \"darkorange\"),\n    (\"dis_sr10\", dis_results, \"Semantic_Recall@10\",   \"Dissonance — Semantic Recall@10\",    \"darkorange\"),\n]\n\nfor fname, results, metric, title, color in plots:\n    fig, ax = plt.subplots(figsize=(6, 3.5))\n    vals = [results[n][metric] for n in NAMES]\n    ax.bar(x, vals, width, color=color)\n    ax.set_xticks(x)\n    ax.set_xticklabels(NAMES, rotation=15, ha='right', fontsize=9)\n    ax.set_ylabel(metric)\n    ax.set_title(title)\n    ax.grid(True, alpha=0.3, axis='y')\n    plt.tight_layout()\n    plt.savefig(f\"/tmp/comp_{fname}.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n\nprint(\"Saved 6 individual plots.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Save & download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "out = {\n    \"metadata\": {\n        \"experiment\": \"comparative_retrieval_analysis\",\n        \"encoder\": \"krag_encoder_aw0.3\",\n        \"encoder_config\": {\n            \"contrastive_weight\": 0.2,\n            \"alignment_weight\": 0.3,\n            \"embedding_dim\": 768,\n            \"num_layers\": 3,\n            \"num_heads\": 4,\n        },\n        \"n_agreement\": len(agreement),\n        \"n_dissonance\": len(dissonance),\n        \"methods\": [\"BM25\", \"Vector-RAG\", \"KRAG (a=1.0)\", \"KRAG (a=0.5)\", \"KRAG (a=0.3)\"],\n        \"device\": DEVICE,\n    },\n    \"agreement\": agr_results,\n    \"dissonance\": dis_results,\n    \"statistical_tests\": stat,\n    \"per_query\": {\n        \"agreement\": {name: {m: scores for m, scores in pq.items()} for name, pq in agr_per_query.items()},\n        \"dissonance\": {name: {m: scores for m, scores in pq.items()} for name, pq in dis_per_query.items()},\n    },\n}\nwith open(\"/tmp/comparative_analysis.json\", \"w\") as f:\n    json.dump(out, f, indent=2)\n\nfrom google.colab.files import download\ndownload(\"/tmp/comparative_analysis.json\")\nfor fname in [\"agr_ap5\", \"agr_ade\", \"agr_sr10\", \"dis_ap5\", \"dis_ade\", \"dis_sr10\"]:\n    download(f\"/tmp/comp_{fname}.png\")\nprint(\"Downloads triggered.\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "comparative_analysis_temp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}