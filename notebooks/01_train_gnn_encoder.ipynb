{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GNN Encoder Training -- Affective-RAG\n\nTrains the Graph Transformer encoder with three loss objectives:\n- **MSE**: Emotion prediction from graph-smoothed representations\n- **Contrastive**: Structural k-differentiation across hop depths\n- **Alignment**: Cosine alignment between GNN output and sentence-BERT space\n\nSweeps `alignment_weight` over {0.05, 0.1, 0.3, 0.5} with `contrastive_weight` fixed at 0.2.\nDownloads the best encoder checkpoint and full sweep results.\n\n### Prerequisites\n| # | Requirement |\n|---|-------------|\n| 1 | Runtime set to **GPU** |\n| 2 | GCP project with GCS bucket containing the dataset |\n| 3 | Fill in `PROJECT_ID` in the auth cell |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies & clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\ndef _pip(*a):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + list(a) + [\"-q\"])\n\n_pip('torch-geometric')\n\nREPO_URL = \"https://github.com/Prashant002-1/Affective-RAG-Recommender-Systems.git\"\n!git clone {REPO_URL} ARAG\n\n_pip(\"-r\", \"ARAG/requirements.txt\")\nprint(\"Done. Restart runtime.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the runtime\n",
    "Go to **Runtime > Restart runtime**, then continue from the next cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate to GCS\n",
    "\n",
    "Replace `YOUR_PROJECT_ID` with your GCP project ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import os\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"          # <-- set this\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "from google.cloud import storage\n",
    "storage.Client(project=PROJECT_ID)      # quick validation\n",
    "print(f\"Authenticated: {PROJECT_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "sys.path.insert(0, \"ARAG/src\")\n",
    "\n",
    "from krag.core.knowledge_graph  import KRAGEncoder\n",
    "from krag.training.gnn_trainer  import GNNTrainer, TrainingConfig, prepare_emotion_ground_truth\n",
    "from krag.data.adapters         import get_adapter, DatasetPath\n",
    "from krag.data.ingestion        import MovieDataLoader, KnowledgeGraphBuilder\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Imports OK | device = {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = get_adapter()          # uses default bucket / base_path\n",
    "\n",
    "# --- 1. Movies (content items) ---\n",
    "print(\"[1/4] Loading movies …\")\n",
    "content_items = MovieDataLoader(adapter).load_all_movies()\n",
    "print(f\"      {len(content_items)} movies\")\n",
    "\n",
    "# --- 2. Knowledge graph (from Neo4j CSVs) ---\n",
    "print(\"[2/4] Building knowledge graph …\")\n",
    "kg = KnowledgeGraphBuilder(adapter).build_from_neo4j_exports()\n",
    "\n",
    "# --- 3. Precomputed node embeddings (768-dim) ---\n",
    "print(\"[3/4] Loading node embeddings …\")\n",
    "node_embeddings = adapter.load_pickle(DatasetPath.NODE_EMBEDDINGS)\n",
    "print(f\"      {len(node_embeddings)} embeddings\")\n",
    "\n",
    "# --- 4. Emotion ground truth ---\n",
    "print(\"[4/4] Loading emotion ground truth …\")\n",
    "movies_df  = adapter.load_movies(vector_ready=True)\n",
    "emotion_gt = prepare_emotion_ground_truth(movies_df)\n",
    "print(f\"      {len(emotion_gt)} movies with labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise & prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config = TrainingConfig(\n    embedding_dim      = 768,\n    num_epochs         = 50,\n    batch_size         = 32,\n    learning_rate      = 1e-4,\n    patience           = 10,\n    contrastive_weight = 0.2,\n    alignment_weight   = 0.3,\n    device             = DEVICE,\n)\n\nencoder = KRAGEncoder(embedding_dim=768, num_layers=3, num_heads=4, dropout=0.1)\ntrainer = GNNTrainer(\n    gnn_encoder=encoder,\n    config=config,\n    knowledge_graph=kg,\n    node_embeddings=node_embeddings,\n)\n\nprint(\"Preparing multi-k training data (one-time) ...\")\ntrain_ds, val_ds = trainer.prepare_multi_k_training_data(\n    content_items        = content_items,\n    node_embeddings      = node_embeddings,\n    emotion_ground_truth = emotion_gt,\n)\nprint(f\"Train: {len(train_ds)}  |  Val: {len(val_ds)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Alignment weight sweep"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import copy, json\n\nALIGNMENT_WEIGHTS = [0.05, 0.1, 0.3, 0.5]\nFIXED_CONTRASTIVE = 0.2\nsweep_results = {}\n\nfor aw in ALIGNMENT_WEIGHTS:\n    print(f\"\\n{'='*60}\")\n    print(f\"  alignment_weight = {aw}\")\n    print(f\"{'='*60}\")\n\n    cfg = TrainingConfig(\n        embedding_dim      = 768,\n        num_epochs         = 50,\n        batch_size         = 32,\n        learning_rate      = 1e-4,\n        patience           = 10,\n        contrastive_weight = FIXED_CONTRASTIVE,\n        alignment_weight   = aw,\n        device             = DEVICE,\n    )\n\n    enc = KRAGEncoder(embedding_dim=768, num_layers=3, num_heads=4, dropout=0.1)\n    trn = GNNTrainer(\n        gnn_encoder=enc,\n        config=cfg,\n        knowledge_graph=kg,\n        node_embeddings=node_embeddings,\n    )\n\n    res = trn.train_multi_k(train_ds, val_ds)\n    sweep_results[aw] = {\n        \"history\": res[\"history\"],\n        \"best_val_loss\": res[\"best_val_loss\"],\n        \"final_epoch\": res[\"final_epoch\"],\n        \"encoder_state\": copy.deepcopy(enc.state_dict()),\n    }\n\nprint(\"\\n\\nSweep complete.\")\nfor aw, r in sweep_results.items():\n    print(f\"  aw={aw:.2f}  best_val={r['best_val_loss']:.4f}  epochs={r['final_epoch']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Alignment sweep results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n\nfor idx, aw in enumerate(ALIGNMENT_WEIGHTS):\n    h = sweep_results[aw][\"history\"]\n    ep = [e[\"epoch\"] for e in h]\n    c = colors[idx]\n    lbl = f\"aw={aw}\"\n\n    axes[0, 0].plot(ep, [e[\"mse_loss\"]         for e in h], color=c, label=lbl)\n    axes[0, 1].plot(ep, [e[\"contrastive_loss\"]  for e in h], color=c, label=lbl)\n    axes[0, 2].plot(ep, [e[\"alignment_loss\"]    for e in h], color=c, label=lbl)\n    axes[1, 0].plot(ep, [e[\"val_loss\"]          for e in h], color=c, label=lbl)\n    axes[1, 1].plot(ep, [e[\"val_cosine\"]        for e in h], color=c, label=lbl)\n    axes[1, 2].plot(ep, [e[\"val_alignment\"]     for e in h], color=c, label=lbl)\n\naxes[0, 0].set_title(\"Train MSE\");        axes[0, 0].set_ylabel(\"Loss\")\naxes[0, 1].set_title(\"Train Contrastive\"); axes[0, 1].set_ylabel(\"Loss\")\naxes[0, 2].set_title(\"Train Alignment\");   axes[0, 2].set_ylabel(\"Loss\")\naxes[1, 0].set_title(\"Val MSE\");           axes[1, 0].set_ylabel(\"Loss\")\naxes[1, 1].set_title(\"Val Cosine Sim\");    axes[1, 1].set_ylabel(\"Cosine\")\naxes[1, 2].set_title(\"Val Alignment\");     axes[1, 2].set_ylabel(\"Loss\")\n\nfor ax in axes.flat:\n    ax.set_xlabel(\"Epoch\")\n    ax.legend(fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"/tmp/alignment_sweep.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\nprint(f\"\\n{'aw':<8} {'best_val':<12} {'epochs':<8} {'mse':<10} {'contr':<10} {'align':<10} {'cos':<10}\")\nprint(\"-\" * 68)\nfor aw in ALIGNMENT_WEIGHTS:\n    r = sweep_results[aw]\n    last = r[\"history\"][-1]\n    print(f\"{aw:<8.2f} {r['best_val_loss']:<12.4f} {r['final_epoch']:<8} \"\n          f\"{last['mse_loss']:<10.4f} {last['contrastive_loss']:<10.4f} \"\n          f\"{last['alignment_loss']:<10.4f} {last['val_cosine']:<10.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Save best encoder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "best_aw = min(sweep_results, key=lambda aw: sweep_results[aw][\"best_val_loss\"])\nprint(f\"Best alignment_weight: {best_aw}  (val_loss={sweep_results[best_aw]['best_val_loss']:.4f})\")\n\noutput = {\n    \"metadata\": {\n        \"experiment\": \"alignment_weight_sweep\",\n        \"contrastive_weight_fixed\": FIXED_CONTRASTIVE,\n        \"alignment_weights_swept\": ALIGNMENT_WEIGHTS,\n        \"best_alignment_weight\": best_aw,\n        \"architecture\": {\n            \"embedding_dim\": 768,\n            \"num_layers\": 3,\n            \"num_heads\": 4,\n            \"dropout\": 0.1,\n        },\n        \"training\": {\n            \"num_epochs\": 50,\n            \"batch_size\": 32,\n            \"learning_rate\": 1e-4,\n            \"patience\": 10,\n        },\n        \"dataset\": {\n            \"train_size\": len(train_ds),\n            \"val_size\": len(val_ds),\n        },\n        \"device\": DEVICE,\n    },\n    \"summary\": {},\n    \"runs\": {},\n}\n\nfor aw in ALIGNMENT_WEIGHTS:\n    r = sweep_results[aw]\n    last = r[\"history\"][-1]\n    output[\"summary\"][str(aw)] = {\n        \"best_val_loss\": r[\"best_val_loss\"],\n        \"final_epoch\": r[\"final_epoch\"],\n        \"final_mse\": last[\"mse_loss\"],\n        \"final_contrastive\": last[\"contrastive_loss\"],\n        \"final_alignment\": last[\"alignment_loss\"],\n        \"final_val_cosine\": last[\"val_cosine\"],\n        \"final_val_alignment\": last[\"val_alignment\"],\n    }\n    output[\"runs\"][str(aw)] = r[\"history\"]\n\nwith open(\"/tmp/alignment_sweep_results.json\", \"w\") as f:\n    json.dump(output, f, indent=2)\n\nfrom google.colab.files import download\n\nfor aw, r in sweep_results.items():\n    path = f\"/tmp/krag_encoder_aw{aw}.pt\"\n    torch.save(r[\"encoder_state\"], path)\n    download(path)\n\ndownload(\"/tmp/alignment_sweep_results.json\")\ndownload(\"/tmp/alignment_sweep.png\")\nprint(\"Downloads triggered.\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_gnn_temp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}