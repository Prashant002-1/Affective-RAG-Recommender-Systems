{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# K-Sweep Evaluation -- Affective-RAG\n\nSweeps subgraph hop depth $k \\in \\{1,2,3,4,5\\}$ using a trained GNN encoder.\nMeasures NDCG@10, AP@5, ADE, and knowledge-score statistics at each depth\nto evaluate structural sensitivity of the graph-augmented retrieval.\n\n### Prerequisites\n| # | Requirement |\n|---|-------------|\n| 1 | Runtime set to **GPU** |\n| 2 | GCP project with GCS bucket containing the dataset |\n| 3 | Trained encoder checkpoint (`krag_encoder.pt`) to upload |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Install & clone"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\ndef _pip(*a):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + list(a) + [\"-q\"])\n\n_pip('torch-geometric')\n\nREPO_URL = \"https://github.com/Prashant002-1/Affective-RAG-Recommender-Systems.git\"\n!git clone {REPO_URL} ARAG\n\n_pip(\"-r\", \"ARAG/requirements.txt\")\nprint(\"Done. Restart runtime.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Restart the runtime\nGo to **Runtime > Restart runtime**, then continue from the next cell."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Authenticate to GCS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import auth\nauth.authenticate_user()\n\nimport os\nPROJECT_ID = \"YOUR_PROJECT_ID\"          # <-- set this\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n\nfrom google.cloud import storage\nstorage.Client(project=PROJECT_ID)\nprint(f\"Authenticated: {PROJECT_ID}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Upload trained encoder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab.files import upload\nprint(\"Upload trained encoder checkpoint (krag_encoder.pt)\")\nuploaded = upload()\nENCODER_PATH = list(uploaded.keys())[0]\nprint(f\"Uploaded: {ENCODER_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, torch, json, numpy as np\nsys.path.insert(0, \"ARAG/src\")\n\nfrom krag.system               import ARAGSystem, ARAGSystemConfig\nfrom krag.training.gnn_trainer  import prepare_emotion_ground_truth\nfrom krag.data.adapters         import get_adapter\nfrom krag.evaluation.synthetic_testset import SyntheticTestSetGenerator\nfrom krag.evaluation.metrics    import (\n    compute_ndcg,\n    compute_affective_precision_at_k,\n    compute_affective_displacement_error,\n)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Imports OK | device = {DEVICE}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialize system"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config = ARAGSystemConfig()\nconfig.vertex_ai_project = PROJECT_ID\nsystem = ARAGSystem(config)\nsystem.initialize()\nstats  = system.load_and_index_data()\nprint(f\"Content items : {stats['content_items']}\")\nprint(f\"KG nodes      : {stats['knowledge_graph_nodes']}\")\nprint(f\"KG edges      : {stats['knowledge_graph_edges']}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load new encoder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import types\n\nsystem.krag_encoder.load_state_dict(\n    torch.load(ENCODER_PATH, map_location=DEVICE)\n)\nsystem.krag_encoder.to(DEVICE)\nsystem.krag_encoder.eval()\n\nsystem.response_generator.generate_response = lambda *a, **kw: \"\"\n\ndef _index_subgraphs(self, content_ids, hops=2):\n    print(f\"Indexing subgraphs for {len(content_ids)} items...\")\n    self.encoder.eval()\n    device = next(self.encoder.parameters()).device\n    with torch.no_grad():\n        for content_id in content_ids:\n            if content_id in self.subgraph_embeddings:\n                continue\n            subgraph = self.kg.extract_subgraph(content_id, hops=hops)\n            if len(subgraph.nodes()) > 0:\n                pyg_data = self.kg.to_pytorch_geometric(subgraph, self.node_embeddings).to(device)\n                embedding = self.encoder.index_subgraph(pyg_data)\n                self.subgraph_embeddings[content_id] = embedding.cpu().numpy()\n\nsystem.subgraph_retriever.index_subgraphs = types.MethodType(_index_subgraphs, system.subgraph_retriever)\n\nprint(\"Encoder loaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load affective signatures & generate test queries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "adapter = get_adapter()\nmovies_df = adapter.load_movies(vector_ready=True)\naff_sigs  = prepare_emotion_ground_truth(movies_df)   # {id: np.array(6)}\ncontent_ids = [str(item.id) for item in system.content_items]\nprint(f\"Affective signatures : {len(aff_sigs)}\")\n\n_result = system.vector_store.semantic_collection.get(include=[\"embeddings\"])\n_emb_map = dict(zip(_result[\"ids\"], _result[\"embeddings\"]))\ncontent_embeddings = np.array([_emb_map[cid] for cid in content_ids])\nprint(f\"Content embeddings   : {content_embeddings.shape}\")\n\ngen = SyntheticTestSetGenerator(\n    content_embedder=system.content_embedder,\n    semantic_threshold=0.5,\n    affective_threshold=0.6,\n    seed=42,\n)\ntest_cases = gen.generate_test_set(\n    content_items=system.content_items,\n    content_embeddings=content_embeddings,\n    movie_affective_signatures=aff_sigs,\n    num_queries=800,\n    min_relevant=3,\n)\nprint(f\"Test queries         : {len(test_cases)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run k-sweep\n\nFor each depth the subgraph index is rebuilt and the full test set is\nevaluated.  `knowledge_score` is extracted from each recommendation to\nverify the graph signal is no longer flat at 0.5.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "EMOTION_ORDER = [\"happiness\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"disgust\"]\nK_VALUES      = [1, 2, 3, 4, 5]\nsweep         = {}\n\nfor k in K_VALUES:\n    print(f\"\\n--- k = {k} ---\", flush=True)\n    system.subgraph_retriever.subgraph_embeddings = {}\n    system.subgraph_retriever.index_subgraphs(content_ids, hops=k)\n\n    ndcg_list, ap5_list, ade_list, ks_list = [], [], [], []\n\n    for tc in test_cases:\n        if isinstance(tc.target_emotions, dict):\n            target_vec  = np.array([tc.target_emotions.get(e, 0.0) for e in EMOTION_ORDER])\n            sliders     = {e: int(round(v * 10)) for e, v in tc.target_emotions.items()}\n        else:\n            target_vec  = np.asarray(tc.target_emotions)\n            sliders     = {e: int(round(v * 10)) for e, v in zip(EMOTION_ORDER, target_vec.tolist())}\n\n        recs  = system.query(\n            query_text=tc.query_text, emotion_sliders=sliders,\n            max_results=10, include_explanation=False\n        )\n        items = recs.get(\"recommendations\", [])\n        if not items:\n            continue\n\n        retrieved_ids = [r[\"content_id\"] for r in items]\n\n        ndcg_list.append(compute_ndcg(retrieved_ids, tc.relevant_items, k=10))\n        ap5_list.append(compute_affective_precision_at_k(\n            retrieved_ids, aff_sigs, target_vec, 5))\n\n        ade = compute_affective_displacement_error(\n            retrieved_ids, aff_sigs, target_vec, 10)\n        if ade != float('inf'):\n            ade_list.append(ade)\n\n        for r in items:\n            ks_list.append(r[\"scores\"][\"knowledge\"])\n\n    sweep[k] = {\n        \"ndcg_10\":              float(np.mean(ndcg_list)) if ndcg_list else 0.0,\n        \"ap_5\":                 float(np.mean(ap5_list))  if ap5_list  else 0.0,\n        \"ade\":                  float(np.mean(ade_list))  if ade_list  else float('inf'),\n        \"knowledge_score_mean\": float(np.mean(ks_list))   if ks_list   else 0.0,\n        \"knowledge_score_std\":  float(np.std(ks_list))    if ks_list   else 0.0,\n        \"n_queries\":            len(ndcg_list),\n    }\n    r = sweep[k]\n    print(f\"  NDCG@10  {r['ndcg_10']:.4f}   AP@5  {r['ap_5']:.4f}   \"\n          f\"ADE  {r['ade']:.4f}   KS  {r['knowledge_score_mean']:.4f} +/- {r['knowledge_score_std']:.4f}\")\n\nprint(\"\\nSweep complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"{'k':>3} | {'NDCG@10':>8} | {'AP@5':>6} | {'ADE':>6} | {'KS mean':>8} | {'KS std':>7} | {'n':>4}\")\nprint(\"-\" * 66)\nfor k in K_VALUES:\n    r = sweep[k]\n    print(f\"{k:>3} | {r['ndcg_10']:>8.4f} | {r['ap_5']:>6.4f} | {r['ade']:>6.4f} | \"\n          f\"{r['knowledge_score_mean']:>8.4f} | {r['knowledge_score_std']:>7.4f} | {r['n_queries']:>4}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Plots"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nks      = list(sweep.keys())\nndcgs   = [sweep[k]['ndcg_10']              for k in ks]\nks_mean = [sweep[k]['knowledge_score_mean'] for k in ks]\nks_std  = [sweep[k]['knowledge_score_std']  for k in ks]\nap5s    = [sweep[k]['ap_5']                 for k in ks]\nades    = [sweep[k]['ade']                  for k in ks]\n\nplot_cfg = {\n    \"ndcg\": (\"Hop depth k\", \"NDCG@10\", \"Ranking quality vs depth\", ndcgs, None, \"steelblue\"),\n    \"ks\":   (\"Hop depth k\", \"Knowledge score\", \"Graph signal (mean +/- std)\", ks_mean, ks_std, \"darkorange\"),\n    \"ap5\":  (\"Hop depth k\", \"AP@5\", \"Affective precision vs depth\", ap5s, None, \"seagreen\"),\n    \"ade\":  (\"Hop depth k\", \"ADE (lower is better)\", \"Displacement error vs depth\", ades, None, \"firebrick\"),\n}\n\nfor name, (xlabel, ylabel, title, vals, errs, color) in plot_cfg.items():\n    fig, ax = plt.subplots(figsize=(5, 3.5))\n    if errs is not None:\n        ax.errorbar(ks, vals, yerr=errs, marker='o', color=color, capsize=4, linewidth=2)\n        ax.axhline(0.5, color='gray', linestyle='--', alpha=0.6, label='Uniform baseline')\n        ax.legend()\n    else:\n        ax.plot(ks, vals, marker='o', color=color, linewidth=2)\n    ax.set_xlabel(xlabel); ax.set_ylabel(ylabel); ax.set_title(title)\n    ax.set_xticks(ks); ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f\"/tmp/k_sweep_{name}.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n\nprint(\"Saved 4 individual plots.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Save & download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "out = {\n    \"metadata\": {\n        \"experiment\": \"k_sweep\",\n        \"encoder\": \"krag_encoder_aw0.3\",\n        \"encoder_config\": {\n            \"contrastive_weight\": 0.2,\n            \"alignment_weight\": 0.3,\n            \"embedding_dim\": 768,\n            \"num_layers\": 3,\n            \"num_heads\": 4,\n        },\n        \"k_values\": K_VALUES,\n        \"n_queries\": len(test_cases),\n        \"device\": DEVICE,\n    },\n    \"results\": {str(k): v for k, v in sweep.items()},\n}\nwith open(\"/tmp/k_sweep_results.json\", \"w\") as f:\n    json.dump(out, f, indent=2)\n\nfrom google.colab.files import download\ndownload(\"/tmp/k_sweep_results.json\")\nfor name in [\"ndcg\", \"ks\", \"ap5\", \"ade\"]:\n    download(f\"/tmp/k_sweep_{name}.png\")\nprint(\"Downloads triggered.\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "k_sweep_temp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}