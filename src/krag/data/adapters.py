"""
GCS Data Adapter for Affective-RAG
Google Cloud Storage adapter with configurable dataset paths
"""

import io
import json
import os
import pandas as pd
from typing import List, Dict, Optional, Any, Generator, Union
from dataclasses import dataclass
from enum import Enum

from google.cloud import storage

from ..core.embeddings import ContentItem
from ..core.emotion_detection import EmotionProfile


# =============================================================================
# GCS Configuration
# =============================================================================

BUCKET_NAME = os.getenv("GCS_BUCKET", "your-gcs-bucket")
BASE_PATH = os.getenv("GCS_BASE_PATH", "Dataset")


class DatasetPath(Enum):
    """Predefined dataset paths (override via config if needed)"""
    
    # Precomputed Artifacts (generated by Colab, loaded everywhere)
    # Version the file when code changes significantly
    EMBEDDINGS = "precomputed/embeddings_v1.npz"
    NODE_EMBEDDINGS = "precomputed/node_embeddings_v1.pkl"
    USER_MOVIE_MAPPING = "precomputed/user_movie_mapping_v1.pkl"  # userId -> set(movieIds)
    VECTOR_INDEX = "precomputed/vector_index_v1"  # For future use
    
    # Neo4j Node Files
    NODES_EMOTIONS = "graph/nodes/emotions.csv"
    NODES_GENRES = "graph/nodes/genres.csv"
    NODES_MOVIES = "graph/nodes/movies.csv"
    NODES_USERS = "graph/nodes/users.csv"
    
    # Neo4j Relationship Files
    REL_MOVIE_GENRE = "graph/relationships/movie_belongs_to_genre.csv"
    REL_MOVIE_EMOTION = "graph/relationships/movie_expresses_emotion.csv"
    REL_MOVIE_SIMILAR = "graph/relationships/movie_similar_emotions.csv"
    REL_USER_EMOTION = "graph/relationships/user_prefers_emotion.csv"
    REL_USER_RATING = "graph/relationships/user_rated_movie.csv"
    
    # Processed Data Files
    MOVIES_VECTOR_READY = "processed/movies.csv"
    USER_EMOTION_PROFILES = "processed/user_emotion_profiles.csv"
    USER_EMOTION_SENSITIVITIES = "processed/user_emotion_sensitivities.csv"
    USER_COMPARISON_PAIRS = "processed/user_comparison_pairs.csv"
    TMDB_METADATA_RAW = "processed/metadata_raw.csv"
    
    # Raw Dataset Files
    EMOTION_DATASET = "raw/emotion_dataset.csv"
    GENOME_SCORES = "raw/genome-scores.csv"
    GENOME_TAGS = "raw/genome-tags.csv"
    LINKS = "raw/links.csv"
    MOVIES_RAW = "raw/movies.csv"
    MOVIES_FINAL = "raw/movies_final.csv"
    RATINGS = "raw/ratings.csv"
    TAGS = "raw/tags.csv"


# =============================================================================
# Schema Definitions
# =============================================================================

@dataclass
class DataSchema:
    """Schema definition for data loading"""
    id_field: str
    title_field: str
    description_field: str
    genre_field: Optional[str] = None
    year_field: Optional[str] = None
    emotion_fields: Optional[Dict[str, str]] = None
    metadata_fields: Optional[List[str]] = None

    def validate(self, data: Dict[str, Any]) -> bool:
        """Check if data contains required fields"""
        required = [self.id_field, self.title_field, self.description_field]
        return all(field in data for field in required)


# Predefined schemas (emotion_fields maps internal label -> column name)
MOVIE_SCHEMA = DataSchema(
    id_field='movieId',
    title_field='title',
    description_field='overview',
    genre_field='genres',
    year_field='release_year',
    emotion_fields={
        'happiness': 'happiness_score',
        'sadness': 'sadness_score',
        'anger': 'anger_score',
        'fear': 'fear_score',
        'surprise': 'surprise_score',
        'disgust': 'disgust_score'
    },
    metadata_fields=['rating', 'vote_count', 'popularity', 'imdbId', 'tmdbId']
)

USER_SCHEMA = DataSchema(
    id_field='userId',
    title_field='userId',
    description_field='userId',
    emotion_fields={
        'happiness': 'happiness_preference',
        'sadness': 'sadness_preference',
        'anger': 'anger_preference',
        'fear': 'fear_preference',
        'surprise': 'surprise_preference',
        'disgust': 'disgust_preference'
    },
    metadata_fields=['total_ratings']
)


# =============================================================================
# GCS Adapter
# =============================================================================

class GCSAdapter:
    """
    Data adapter for Google Cloud Storage.
    Optimized for the ARAG dataset structure.
    """

    def __init__(self, bucket_name: str = BUCKET_NAME, base_path: str = BASE_PATH):
        self.bucket_name = bucket_name
        self.base_path = base_path
        self._client: Optional[storage.Client] = None
        self._bucket: Optional[storage.Bucket] = None

    @property
    def client(self) -> storage.Client:
        """Lazy initialization of GCS client"""
        if self._client is None:
            self._client = storage.Client()
        return self._client

    @property
    def bucket(self) -> storage.Bucket:
        """Lazy initialization of bucket reference"""
        if self._bucket is None:
            self._bucket = self.client.bucket(self.bucket_name)
        return self._bucket

    def _full_path(self, path: Union[str, DatasetPath]) -> str:
        """Construct full GCS path"""
        if isinstance(path, DatasetPath):
            path = path.value
        return f"{self.base_path}/{path}"

    def exists(self, path: Union[str, DatasetPath]) -> bool:
        """Check if a file exists in GCS"""
        blob = self.bucket.blob(self._full_path(path))
        return blob.exists()

    def list_files(self, prefix: str = "") -> List[str]:
        """List files under a prefix"""
        full_prefix = f"{self.base_path}/{prefix}" if prefix else self.base_path
        blobs = self.client.list_blobs(self.bucket_name, prefix=full_prefix)
        return [blob.name for blob in blobs]

    # -------------------------------------------------------------------------
    # Data Loading Methods
    # -------------------------------------------------------------------------

    def load_csv(
        self,
        path: Union[str, DatasetPath],
        chunk_size: Optional[int] = None,
        usecols: Optional[List[str]] = None,
        dtype: Optional[Dict[str, Any]] = None
    ) -> Union[pd.DataFrame, Generator[pd.DataFrame, None, None]]:
        """
        Load CSV file from GCS.
        
        Args:
            path: File path or DatasetPath enum
            chunk_size: If provided, returns generator of DataFrames
            usecols: Columns to load (memory optimization)
            dtype: Column dtypes (memory optimization)
        
        Returns:
            DataFrame or Generator of DataFrames if chunk_size is set
        """
        blob = self.bucket.blob(self._full_path(path))
        content = blob.download_as_text()
        
        if chunk_size:
            return pd.read_csv(
                io.StringIO(content),
                chunksize=chunk_size,
                usecols=usecols,
                dtype=dtype
            )
        return pd.read_csv(io.StringIO(content), usecols=usecols, dtype=dtype)

    def load_json(self, path: Union[str, DatasetPath]) -> Any:
        """Load JSON file from GCS"""
        blob = self.bucket.blob(self._full_path(path))
        content = blob.download_as_text()
        return json.loads(content)

    def load_jsonl(self, path: Union[str, DatasetPath]) -> Generator[Dict, None, None]:
        """Load JSONL file from GCS as generator"""
        blob = self.bucket.blob(self._full_path(path))
        content = blob.download_as_text()
        for line in content.split('\n'):
            if line.strip():
                yield json.loads(line)

    def load_numpy(self, path: Union[str, DatasetPath]) -> Dict[str, Any]:
        """
        Load numpy .npz file from GCS.
        
        Args:
            path: File path or DatasetPath enum (should end in .npz)
            
        Returns:
            Dict-like object with numpy arrays (use ['key'] to access)
        """
        import numpy as np
        blob = self.bucket.blob(self._full_path(path))
        content = blob.download_as_bytes()
        return np.load(io.BytesIO(content))

    def save_numpy(self, path: Union[str, DatasetPath], **arrays) -> None:
        """
        Save numpy arrays to GCS as .npz file.
        
        Args:
            path: File path or DatasetPath enum
            **arrays: Named arrays to save (e.g., semantic=arr1, emotion=arr2)
        """
        import numpy as np
        buffer = io.BytesIO()
        np.savez(buffer, **arrays)
        buffer.seek(0)
        
        blob = self.bucket.blob(self._full_path(path))
        blob.upload_from_file(buffer, content_type='application/octet-stream')

    def load_pickle(self, path: Union[str, DatasetPath]) -> Any:
        """
        Load pickle file from GCS.
        
        Args:
            path: File path or DatasetPath enum (should end in .pkl)
            
        Returns:
            Unpickled Python object
        """
        import pickle
        blob = self.bucket.blob(self._full_path(path))
        content = blob.download_as_bytes()
        return pickle.loads(content)

    def save_pickle(self, path: Union[str, DatasetPath], obj: Any) -> None:
        """
        Save Python object to GCS as pickle file.
        
        Args:
            path: File path or DatasetPath enum
            obj: Python object to pickle
        """
        import pickle
        buffer = io.BytesIO()
        pickle.dump(obj, buffer)
        buffer.seek(0)
        
        blob = self.bucket.blob(self._full_path(path))
        blob.upload_from_file(buffer, content_type='application/octet-stream')

    # -------------------------------------------------------------------------
    # Content Item Loading
    # -------------------------------------------------------------------------

    def load_content_items(
        self,
        path: Union[str, DatasetPath],
        schema: DataSchema,
        chunk_size: int = 1000
    ) -> Generator[ContentItem, None, None]:
        """
        Load data as ContentItem objects for embedding/indexing.
        
        Args:
            path: File path or DatasetPath enum
            schema: DataSchema defining field mappings
            chunk_size: Rows to process at a time
        
        Yields:
            ContentItem objects
        """
        for chunk in self.load_csv(path, chunk_size=chunk_size):
            for _, row in chunk.iterrows():
                item = self._row_to_content_item(row.to_dict(), schema)
                if item:
                    yield item

    def _row_to_content_item(
        self,
        row: Dict[str, Any],
        schema: DataSchema
    ) -> Optional[ContentItem]:
        """Convert a row to ContentItem using schema mapping"""
        try:
            item_id = str(row.get(schema.id_field, ''))
            title = str(row.get(schema.title_field, ''))
            description = str(row.get(schema.description_field, '') or '')

            if not item_id or not title:
                return None

            # Parse genres
            genres = []
            if schema.genre_field and schema.genre_field in row:
                genre_val = row[schema.genre_field]
                if isinstance(genre_val, list):
                    genres = genre_val
                elif isinstance(genre_val, str):
                    genres = [g.strip() for g in genre_val.split('|') if g.strip()]

            # Parse year
            year = None
            if schema.year_field and schema.year_field in row:
                try:
                    year = int(row[schema.year_field])
                except (ValueError, TypeError):
                    pass

            # Parse emotions
            emotions = None
            if schema.emotion_fields:
                emotion_dict = {}
                for emotion_name, field_name in schema.emotion_fields.items():
                    if field_name in row and row[field_name] is not None:
                        try:
                            emotion_dict[emotion_name] = float(row[field_name])
                        except (ValueError, TypeError):
                            pass
                if emotion_dict:
                    emotions = EmotionProfile.from_dict(emotion_dict)

            # Collect metadata
            metadata = {}
            if schema.metadata_fields:
                for field_name in schema.metadata_fields:
                    if field_name in row and pd.notna(row[field_name]):
                        metadata[field_name] = row[field_name]

            return ContentItem(
                id=item_id,
                title=title,
                description=description,
                genres=genres,
                year=year,
                emotions=emotions,
                metadata=metadata
            )
        except Exception:
            return None

    # -------------------------------------------------------------------------
    # Convenience Methods for ARAG Datasets
    # -------------------------------------------------------------------------

    def load_movies(self, vector_ready: bool = True) -> pd.DataFrame:
        """Load movie dataset"""
        path = DatasetPath.MOVIES_VECTOR_READY if vector_ready else DatasetPath.MOVIES_FINAL
        return self.load_csv(path)

    def load_movie_content_items(self, chunk_size: int = 1000) -> Generator[ContentItem, None, None]:
        """Load movies as ContentItem objects for embedding"""
        return self.load_content_items(
            DatasetPath.MOVIES_VECTOR_READY,
            MOVIE_SCHEMA,
            chunk_size=chunk_size
        )

    def load_ratings(self, chunk_size: Optional[int] = 100000) -> Union[pd.DataFrame, Generator]:
        """Load ratings dataset (large file, chunking recommended)"""
        return self.load_csv(DatasetPath.RATINGS, chunk_size=chunk_size)

    def load_user_profiles(self) -> pd.DataFrame:
        """Load user emotion profiles"""
        return self.load_csv(DatasetPath.USER_EMOTION_PROFILES)

    def load_user_sensitivities(self) -> pd.DataFrame:
        """Load user emotion sensitivities"""
        return self.load_csv(DatasetPath.USER_EMOTION_SENSITIVITIES)

    def load_emotion_dataset(self) -> pd.DataFrame:
        """Load emotion dataset"""
        return self.load_csv(DatasetPath.EMOTION_DATASET)

    def load_genome_data(self) -> tuple[pd.DataFrame, pd.DataFrame]:
        """Load genome scores and tags"""
        scores = self.load_csv(DatasetPath.GENOME_SCORES)
        tags = self.load_csv(DatasetPath.GENOME_TAGS)
        return scores, tags

    # Neo4j data loaders
    def load_neo4j_nodes(self, node_type: str) -> pd.DataFrame:
        """Load Neo4j node CSV by type (emotions, genres, movies, users)"""
        path_map = {
            'emotions': DatasetPath.NODES_EMOTIONS,
            'genres': DatasetPath.NODES_GENRES,
            'movies': DatasetPath.NODES_MOVIES,
            'users': DatasetPath.NODES_USERS,
        }
        if node_type not in path_map:
            raise ValueError(f"Unknown node type: {node_type}. Valid: {list(path_map.keys())}")
        return self.load_csv(path_map[node_type])

    def load_neo4j_relationships(self, rel_type: str) -> pd.DataFrame:
        """
        Load Neo4j relationship CSV by type.
        
        Valid types: movie_genre, movie_emotion, movie_similar, 
                    user_emotion, user_rating
        """
        path_map = {
            'movie_genre': DatasetPath.REL_MOVIE_GENRE,
            'movie_emotion': DatasetPath.REL_MOVIE_EMOTION,
            'movie_similar': DatasetPath.REL_MOVIE_SIMILAR,
            'user_emotion': DatasetPath.REL_USER_EMOTION,
            'user_rating': DatasetPath.REL_USER_RATING,
        }
        if rel_type not in path_map:
            raise ValueError(f"Unknown relationship type: {rel_type}. Valid: {list(path_map.keys())}")
        return self.load_csv(path_map[rel_type])

    # -------------------------------------------------------------------------
    # Data Writing (for saving processed outputs)
    # -------------------------------------------------------------------------

    def save_csv(self, df: pd.DataFrame, path: str, **kwargs) -> str:
        """
        Save DataFrame to GCS as CSV.
        
        Args:
            df: DataFrame to save
            path: Destination path (relative to base_path)
            **kwargs: Additional arguments for to_csv()
        
        Returns:
            Full GCS URI of saved file
        """
        blob = self.bucket.blob(self._full_path(path))
        blob.upload_from_string(
            df.to_csv(index=False, **kwargs),
            content_type='text/csv'
        )
        return f"gs://{self.bucket_name}/{self._full_path(path)}"

    def save_json(self, data: Any, path: str) -> str:
        """Save data to GCS as JSON"""
        blob = self.bucket.blob(self._full_path(path))
        blob.upload_from_string(
            json.dumps(data, indent=2),
            content_type='application/json'
        )
        return f"gs://{self.bucket_name}/{self._full_path(path)}"


# =============================================================================
# Module-level convenience function
# =============================================================================

def get_adapter(
    bucket_name: str = BUCKET_NAME,
    base_path: str = BASE_PATH
) -> GCSAdapter:
    """Get a configured GCS adapter instance"""
    return GCSAdapter(bucket_name=bucket_name, base_path=base_path)